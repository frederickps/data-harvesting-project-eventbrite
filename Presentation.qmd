---
title: "Presentation"
subtitle: "Harvesting data from Eventbrite from around the world"
author: "Eric & Frederick"
date: 2024-03-22
format: 
  revealjs:
    theme: simple
    transition: slide
    transition-speed: fast
    embed-resources: TRUE
editor: visual
logo: https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-Flat.png
---

## Objectives of our project

```{r}
#|echo: false

library(tidyverse)
library(rvest)
library(xml2)
library(jsonlite)
library(httr)

# Analysis
library(tm)
library(tidytext)
library(forcats)
```

-   gather information of events from cities around the world
-   build a robust scraper to gather information on any given data
-   facilitate usage of data by providing a ready-made csv-file

# Functions

## Getting the url right

![](images/clipboard-1765564271.png)

-   "<https://www.eventbrite.es/d/>"

-   "spain--madrid"

-   "/events-tomorrow/"

-   "?page=1"

## load_cities_list {style="font-size: 80%;"}

-   preparing a city name input for the correct format in the link on eventbrite.es

```{r}
#| echo: true

load_cities_list <- function() {
  
  cities_list <- tribble(~input, ~text,
                         "madrid", "spain--madrid",
                         "roma", "italy--roma",
                         "london", "united-kingdom--london",
                         "paris", "france--paris",
                         "barcelona", "spain--barcelona",
                         "san francisco", "ca--san-francisco",
                         "berlin", "germany--berlin",
                         "amsterdam", "netherlands--amsterdam",
                         "warsaw", "poland--warszawa",
                         "lyon", "france--lyon", 
                         "marseille", "france--marseille", 
                         "sevilla", "spain--sevilla", 
                         "vigo", "spain--vigo", 
                         "malaga", "spain--malaga", 
                         "zaragoza", "spain--zaragoza", 
                         "marbella", "spain-marbella", 
                         "bilbao", "spain--bilbao", 
                         "san sebastian", "spain--donostia-san-sebastián", 
                         "cadiz", "spain--cádiz", 
                         "granada", "spain--granada", 
                         "valladolid", "spain--valladolid",
                         "santiago", "chile--santiago",
                         "buenos aires", "argentina--buenos-aires",
                         "sydney", "australia--sydney",
                         "lima", "peru--lima",
                         "glasgow", "united-kingdom--glasgow",
                         "dublin", "ireland--dublin",
                         "lisbon", "portugal--lisboa"
                         
  )
  
  return(cities_list)
}
```

## create_url

-   using `load_cities_list` to create a correctly formatted link

```{r}
#| echo: true

create_url <- function(city) {
  
  cities_list <- load_cities_list()  # loading city list into function
  
  url_city <- cities_list$text[cities_list$input == city] # returns the city name in Eventbrite's url format 
  
  url_root <- "https://www.eventbrite.es/d/"
  url_suffix <- "/events--tomorrow/" # you can change this to `/events_today/`
  # url_suffix <- "/events--today/" 
  
  # combines the parts of the url to create full url 
  url <- paste0(url_root, url_city, url_suffix)
  
  return(url)
}
```

## count_pages

-   extracting the number of pages

```{r}
#| echo: true

count_pages <- function(link){
  
  total_pages <- link |> 
    read_html() |> 
    xml_find_all("//li[@class='eds-pagination__navigation-minimal eds-l-mar-hor-3']") |> 
    xml_text() |> 
    str_extract_all("\\d+$") |> 
    pluck(1) # extracts first item 
  
  total_pages <- as.numeric(total_pages)
  return(total_pages) # returns the number of available pages as an integer
}
```

## Where?

![](images/clipboard-2423281174.png)

## Where to get the data from?

::: columns
::: {.column width="30%"}
First idea...

![](images/clipboard-3877888909.png)
:::

::: {.column width="70%"}
... better idea

![](images/clipboard-824950995.png)
:::
:::

## get_event_links

-   collecting all individual event links for a given website

```{r}
#| echo: true

get_event_links <- function(link, page_num) {
  
  link_with_page_num <- paste0(link, "?page=", page_num) # adds page number info to url 
  
  html_link <- link_with_page_num |> 
    read_html() |>
    xml_find_all("//section[@class='event-card-details']//div[@class='Stack_root__1ksk7']//a") |> 
    xml_attr("href")
  
  # remove the repeating links that are automatically scraped
  links <- unique(html_link)
  
  return(links) # returns character vector of links
}
```

## get_all_event_links {style="font-size: 90%;"}

-   combining `count_pages` and `get_event_links` to loop through all pages to get every single event link

```{r}
#| echo: true

get_all_event_links <- function(link) {
  
  all_event_links <- c() # creates empty vector
  
  pages_count <- count_pages(link = link) # gets number of available pages at this link 
  # pages_count <- min(10, pages_count) # don't pull more than 10 pages at once
  
  for (i in 1:pages_count) {
    
    Sys.sleep(2)
    # loops through number of pages 
    # gets a new batch of event links from page i 
    links <- get_event_links(link = link, page_num = i)
    
    # adds it to the full vector of titles 
    all_event_links <- c(all_event_links, links) 
  }
  
  # returns only event links that are not NA and that are unique
  return(unique(all_event_links[!is.na(all_event_links)]))
  
}
```

## and finally... get_event_info {style="font-size: 70%;"}

-   scraper looping over the list of all event links gathered from `get_all_event_links`

-   "cityname" as manual input defining the "City" column and file name

```{r}
#| echo: true

get_event_info <- function(link, cityname) {
  
  # data frame for the city with all events 
  shared_df <- data.frame()
  
  # stores all the links we will scrape as character vector
  all_links <- get_all_event_links(link)
  
  # name of the CSV file that will be exported from shared_df
  file_path <- paste0(cityname, format(Sys.Date()+1, "%Y%m%d"), ".csv")
  
  num_all_links <- length(all_links) # number of links/events
  j <- 0 # variable for counting links being scraped 
  
    # Loop over each link in 'all_links'
for (i in seq_along(all_links)) {
  
  Sys.sleep(1)
  
  j <- j + 1 # adds one each loop for counting progress
  
  event_link <- all_links[i]
  
  html <- event_link |> read_html()
  


# Read HTML and extract duration if there is information, if not NA
  
  tryCatch({shared_df[i, "Duration"] <- if (html |>
                                            xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
                                            length() == 0) {
    NA
    } else {
      html |>
        xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
        xml_text() %>%
        .[[1]]
      }
  }, error = function(e) {
    shared_df[i, "Duration"] <- NA
    })
  
  # Extract ticket type
  tryCatch({
    shared_df[i, "Ticket_Type"] <- if (html |>
                                       xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
                                       length() == 0) {
      "Sold out"
      } else {
        html |>
          xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
          xml_text() %>%
          .[[2]]
        }
    }, error = function(e) {
      shared_df[i, "Ticket_Type"] <- NA})
  
  # Extract refund policy. If no info, store as NA
  
  tryCatch({
    shared_df[i, "Refund_Policy"] <- if (html |>
                                         xml_find_all("//div[@class = 'Layout-module__module___2eUcs Layout-module__refundPolicy___fQ8I7']//section[@class = 'event-details__section']/div") |>
                                         length() == 0) {
      NA
      } else {
        html |>
          xml_find_all("//div[@class = 'Layout-module__module___2eUcs Layout-module__refundPolicy___fQ8I7']//section[@class = 'event-details__section']/div") |>
          xml_text() %>%
          .[[2]]
        }
    }, error = function(e) {
      shared_df[i, "Refund_Policy"] <- NA
      })
# Extract description. If no info, save as NA
  tryCatch({
    shared_df[i, "Description"] <- if (html |>
                                       xml_find_all("//div[@class = 'eds-text--left']//p") |>
                                       length() == 0){
      NA
      } else {
        html |>
          xml_find_all("//div[@class = 'eds-text--left']//p") |>
          xml_text() |>
          discard(~.x == "") |>
          paste(collapse = " ")
      }
    }, error = function(e) {
      shared_df[i, "Description"] <- NA
      })
  
  # pull json data
  
  json <-
    html |>
    xml_find_all("//script[@type='application/ld+json']") |>
    pluck(1) |>
    xml_text()
  
  tryCatch({json <- fromJSON(json)},
         error = function(e) {
           json <- NA})
  
  tryCatch({shared_df[i, "LowPrice"] <- json$offers$lowPrice[1]},
           error = function(e) {
             shared_df[i, "LowPrice"] <- NA})
  tryCatch({shared_df[i, "HighPrice"] <- json$offers$highPrice[1]},
           error = function(e) {
             shared_df[i, "HighPrice"] <- NA})
  tryCatch({shared_df[i, "Currency"] <- json$offers$priceCurrency[1]},
           error = function(e) {
             shared_df[i, "Currency"] <- NA})
  tryCatch({shared_df[i, "Organizer"] <- json$organizer$name},
           error = function(e) {
             shared_df[i, "Organizer"] <- NA})
  tryCatch({shared_df[i, "EventStatus"] <- json$eventStatus},
           error = function(e) {
             shared_df[i, "EventStatus"] <- NA})
  tryCatch({shared_df[i, "StartTime"] <- lubridate::as_datetime(json$startDate)},
           error = function(e) {
             shared_df[i, "StartTime"] <- NA})
  tryCatch({shared_df[i, "EndTime"] <- lubridate::as_datetime(json$endDate)},
           error = function(e) {
             shared_df[i, "EndTime"] <- NA})
  tryCatch({shared_df[i, "Title"] <- json$name},
           error = function(e) {
             shared_df[i, "Title"] <- NA})
  tryCatch({shared_df[i, "Subtitle"] <- json$description},
           error = function(e) {
             shared_df[i, "Subtitle"] <- NA})
  tryCatch({shared_df[i, "url"] <- json$url},
           error = function(e) {
             shared_df[i, "url"] <- NA})
  
  shared_df[i, "City"] <- cityname # city variable for when we complile data 
  
  # calculating and reporting the progress of the scraper
  print(paste(round(100 * j/num_all_links, 2), "% completed"))
  
  # read the CSV file with all the previously saved events and save as `res`
  res <- try(read_csv(file_path, show_col_types = FALSE), silent = TRUE)
  
  if (inherits(res, "try-error")) {
    # Save the data frame we scraped above
    print("File doesn't exist; Creating it")
    write_csv(shared_df, file_path)
    } else {
      # If the file was read successfully, append the
      # new rows and save the file again
      combined_df <- bind_rows(res, shared_df[i,])
      write_csv(combined_df, file_path)
    }
  rm(res) # removed `res` to save memory space 
}
  }
```

## Let's try it out

-   we only need two functions to scrape all events from a given city: `create_url` and `get_event_info`

::: columns
::: {.column width="50%"}
Create a link for a city you are interested in:

```{r}
#| echo: true

l <- create_url("madrid")

l

count_pages(l)
```
:::

::: {.column width="50%"}
Use that link to get ALL events:

```{r, eval=FALSE}
#| echo: true

get_event_info(l, "madrid") 
```
:::
:::

# Analysis

## Average number of daily events

```{r}
#| echo: false

# Setting the directory path where csv files are located
folder_path <- "data"

# Getting a list of csv files in the folder
csv_files <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE)

# Initializing an empty list to store data frames
all_data <- list()

# Looping through each csv file and read it into a data frame
for (file in csv_files) {

  data <- read.csv(file)
  
  # Append the data frame to the list
  all_data[[length(all_data) + 1]] <- data
}

# Combine all data frames into a single object
combined_data <- bind_rows(all_data)

# Fixed some spelling mistakes in variable City
combined_data <- combined_data |> 
  mutate(City = case_when(
    City == "buenos_aires" ~ "buenos aires",
    City == "sevila" ~ "sevilla",
    .default = City
  ))

```

```{r}
#| echo: false

combined_data$Date <- as.Date(combined_data$StartTime)

average_n_events <- 
  combined_data |> 
  group_by(City, Date)  |> 
  summarise(daily_events = n())  |> 
  group_by(City)  |> 
  summarise(average_daily_events = mean(daily_events))

ggplot(average_n_events, aes(x = City, y = average_daily_events)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Average Number of Daily Events per City",
       y ="",
       x = "",
       caption = "Data collected between 09.03.2024 and 14.03.2024")+
  geom_text(aes(label = round(average_daily_events, 2)), vjust = -0.5, size = 2.2) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## How expensive are events on average (1)?

```{r}
#| echo: false

# Exchange rates (12.03.2024)
exchange_rates <- data.frame(
  Currency = c("USD", "GBP", "EUR", "ARS", "SGD", "CAD", "CHF", "AUD", "NZD", "PLN"),  # Currency codes
  Exchange_Rate = c(0.91, 1.17, 0, 0.0011, 0.69, 0.68, 1.04, 0.60, 0.56, 0.23)  # Corresponding exchange rates to Euros
)

# Merging exchange rate data with combined_data based on the Currency column
combined_data <- merge(combined_data, exchange_rates, by = "Currency", all.x = TRUE)

convert_to_euros <- function(price, currency, exchange_rate) {
  if (is.na(currency)) {
    return(NA)  # Return NA if the currency is missing
  } else if (currency == "EUR") {
    return(price)  # If the currency is already EUR, return the price as it is
  } else {
    return(price * exchange_rate)  # Convert price to Euros using the exchange rate
  }
}

# Adding the converted prices to the main dataframe
combined_data$Low_Price_Euros <- mapply(convert_to_euros, combined_data$LowPrice, combined_data$Currency, 
                                    combined_data$Exchange_Rate)
combined_data$High_Price_Euros <- mapply(convert_to_euros, combined_data$HighPrice, combined_data$Currency, 
                                    combined_data$Exchange_Rate)
```

```{r}
#| echo: false

average_low_price <- 
  combined_data |> 
  drop_na(Low_Price_Euros) |> 
  group_by(City)  |> 
  summarise(average_price = mean(Low_Price_Euros))

ggplot(average_low_price, aes(x = City, y = average_price)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = paste0(round(average_price, 2), "€")), 
            vjust = -0.5, size = 2) +  
  labs(title = "Average Low Price per Event by City",
       x = "City",
       y = "") +
  scale_y_continuous(labels = function(x) paste0(x, "€")) +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## How expensive are events on average (2)?

```{r}
#| echo: false

average_high_price <- 
  combined_data |> 
  drop_na(High_Price_Euros) |> 
  group_by(City)  |> 
  summarise(average_price = mean(High_Price_Euros))

ggplot(average_high_price, aes(x = City, y = average_price)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = paste0(round(average_price, 2), "€")), 
            vjust = -0.5, size = 2) +  
  labs(title = "Average High Price per Event by City",
       x = "City",
       y = "") +
  scale_y_continuous(labels = function(x) paste0(x, "€")) +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Appendix - Standardizing time

## Challenge

-   time extracted from JSON uses local system time

-   time extracted does not observe one single format (military and US time)

![](images/clipboard-349616894.png)

-   many different formats used by hosts

## Pre-process data {style="font-size: 70%;"}

-   Extracting time from individual webpage and standardizing format

-   Output: dd:dd - dd:dd(am\|pm\|AM\|PM)

```{r}
#| echo: true

filter_list <- 
  function(list) {
  return(list[sapply(list, function(x) is.character(x) && nchar(x) > 0)])}

pre_convert_time <- function (html) {if (
    html |> 
    xml_find_all("//div[@class = 'date-info']//span") |>
    length() == 0) {
      NA
  } 
  else {
    html |> 
      xml_find_all("//div[@class = 'date-info']//span") |> 
      xml_text() %>%
      filter_list() %>%
  str_extract_all("\\b(?:\\d+:\\d+(?:pm|PM|am|AM)?|(?:\\d+:)?\\d+pm|\\d+PM|\\d+am|\\d+AM|\\d+ - \\d+:\\d+(?:pm|PM|am|AM)?|\\d+ - \\d+(?:pm|PM|am|AM)?)") %>%
  .[[1]] %>%
  paste0(collapse = " - ")  %>% 
  {if (str_detect(., "\\b\\d{1}(?![0-9]|:)(?:pm|PM|am|AM)")) { # Matches single-digit numbers not followed by another digit or colon, followed by "pm" or "am"
    gsub("\\b(\\d{1})(pm|PM|am|AM)", "0\\1:00\\2", .)
  } else if (str_detect(., "\\b(\\d{1}):?")) { # Matches singular numbers followed by a colon
    gsub("\\b(\\d{1}):", "0\\1:", .)
  } else if (str_detect(., "\\d{1} - \\d{1}(?:pm|PM|am|AM)?")){
    gsub("(\\d{1,2}) - (\\d{1,2})(pm|PM|am|AM)", "0\\1:00 - 0\\2:00\\3", .)
  } else if (str_detect(., "\\d{1,2}(?:pm|PM|am|AM)?")){
    if_else(str_detect(., "pm|PM"), gsub("(\\d{1,2})(pm|PM|am|AM)?", "0\\1:00 - 00:00pm", .), 
            gsub("(\\d{1,2})(pm|PM|am|AM)?", "0\\1:00 - 00:00", .))
  } else if (str_detect(., "\\d{1}")) {
    gsub("(\\d{1})", "0\\1:00", .)
  } else {
    paste(.)
  }
  } %>%
  gsub("\\b(\\d)( - |$)", "0\\1:00\\2", .)%>%
  gsub("^\\b(\\d{2}:\\d{2})(pm|am|PM|AM)$", "\\1 - 00:00\\2", .) %>%
  gsub("^(\\d{2}:\\d{2})$", "\\1 - 00:00\\2", .)%>%
  gsub("(?<=\\s\\d{2})(pm|am|PM|AM)\\b", ":00\\1", ., perl = TRUE)%>%
  gsub("(\\d{1,2})(am|pm|AM|PM)\\s", "\\1:00\\2 ", ., perl = TRUE) %>%
  gsub("\\b(\\d):(\\d{2}(am|pm|AM|PM)?)\\b", "0\\1:\\2", .) %>%
  gsub("(?<!\\d:)(\\b\\d{2})\\b\\s", "\\1:00 ", ., perl = TRUE) %>%
  gsub("(pm|am) - ", " - ", .)%>%
  gsub("(am|AM)$", "pm", .)
  }
  
}
```

## Converting time {style="font-size: 90%;"}

-   converting starting time into military time if provided in US-time

```{r}
#| echo: true

convert_time_format <- function(time_string) {
  # Check if "pm" or "PM" is present in the input string
  if (is.na(time_string)) {
    return(NA)
  } else {
    if(str_detect(time_string, "pm|PM")) {
    # Extract hours and minutes from the input string
    time_parts <- str_match(time_string, "(\\d+):(\\d+) - (\\d+):(\\d+)(pm)?")
    start_hour <- as.integer(time_parts[2])
    start_minute <- as.integer(time_parts[3])
    end_hour <- as.integer(time_parts[4])
    end_minute <- as.integer(time_parts[5])
    
    # Convert to 24-hour format
    if (start_hour != 12) {
      start_hour <- start_hour + 12
    }
    
    # Return the start time string in the desired format
    result <- sprintf("%02d:%02d", start_hour, start_minute)
    } else {
      start_time <- str_match(time_string, "(\\d+):(\\d+) - (\\d+):(\\d+)")
      start_hour <- as.integer(start_time[2])
      start_minute <- as.integer(start_time[3])
      
      time_string <- sprintf("%02d:%02d", start_hour, start_minute)
      # Return the original time string if "pm" or "PM" is not present
      result <- time_string
    }
    
    return(result)
  }
}
```
