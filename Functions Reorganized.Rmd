---
title: "Functions"
author: "Eric, Frederic"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
library(xml2)
library(jsonlite)
```

# Functions 

## Load cities 

Currently includes Madrid, Rome, London, Lyon, Paris, Barcelona, Torino, Sevilla, and CÃ¡diz. 

```{r}
load_cities_list <- function() {
  
  cities_list <- tribble(~input, ~text,
                         "madrid", "spain--madrid",
                         "roma", "italy--roma",
                         "london", "united-kingdom--london",
                         "paris", "france--paris",
                         "barcelona", "spain--barcelona",
                         "san francisco", "ca--san-francisco",
                         "berlin", "germany--berlin",
                         "amsterdam", "netherlands--amsterdam",
                         "warsaw", "poland--warszawa",
                         
  )
  
  return(cities_list)
}
```


## Create URL 

```{r}
create_url <- function(city) {
  
  cities_list <- load_cities_list()  # loading city list into function
  
  url_city <- cities_list$text[cities_list$input == city]
  
  url_root <- "https://www.eventbrite.es/d/"
  url_suffix <- "/events--tomorrow/"
  
  url <- paste0(url_root, url_city, url_suffix)
  
  return(url)
}
```


## Count pages 

```{r}
count_pages <- function(link){
  
  total_pages <- link |> 
    read_html() |> 
    xml_find_all("//li[@class='eds-pagination__navigation-minimal eds-l-mar-hor-3']") |> 
    xml_text() |> 
    str_extract_all("\\d+$") |> 
    pluck(1)
  
  total_pages <- as.numeric(total_pages)
  return(total_pages) # returns the number of available pages as an integer
}
```


### Get event titles 
> not really needed now that we have get all links 

### Get all titles 
> not really needed now that we have get all links 

## Get event links 

This function returns all the links for one page, given in the arguement. 

```{r}
get_event_links <- function(link, page_num) {
  
  link_with_page_num <- paste0(link, "?page=", page_num) # adds page number info to url 
  
  html_link <- link_with_page_num |> 
    read_html() |>
    xml_find_all("//section[@class='event-card-details']//div[@class='Stack_root__1ksk7']//a") |> 
    xml_attr("href")
  
  # remove the repeating titles 
  links <- unique(html_link)
  
  return(links)
}
```

## Get all event links 

This function returns all the event links for all pages for a city. 

Includes `count_pages()` and `get_event_links()`. 

Also includes `Sys.sleep(2)`. 

```{r}
get_all_event_links <- function(link) {
  
  all_event_links <- c() # creates empty vector
  
  pages_count <- count_pages(link = link) # gets number of available pages at this link 
  # pages_count <- min(10, pages_count) # don't pull more than 10 pages at once
  
  for (i in 1:pages_count) {
    
    Sys.sleep(2)
    # loops through number of pages 
    # gets a new batch of event links from page i 
    links <- get_event_links(link = link, page_num = i)
    
    # adds it to the full vector of titles 
    all_event_links <- c(all_event_links, links) 
  }
  
  # returns only event links that are not NA and that are unique
  return(unique(all_event_links[!is.na(all_event_links)]))
  
}
```



## Create file path 

Extracts info from html for duration, ticket type, refund policy, and description. 


```{r}
file_path <- ""
```


## Get all event info 

This function returns all info needed for all pages of a city's events. 

```{r}

get_event_info <- function(link, cityname) {
  
  shared_df <- data.frame()
  
  all_links <- get_all_event_links(link)
  
  file_path <- paste0(cityname, format(Sys.Date()+1, "%Y%m%d"),".csv")
  
  num_all_links <- length(all_links) 
  j <- 0
    # Loop over each link in 'test_object'
for (i in seq_along(all_links)) {
  Sys.sleep(1)
  
  j <- j + 1
  
  event_link <- all_links[i]
  
  html <- event_link |> read_html()
  

# Read HTML and extract duration if there is information, if not NA
  
  tryCatch({shared_df[i, "Duration"] <- if (html |>
                                            xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
                                            length() == 0) {
    NA
    } else {
      html |>
        xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
        xml_text() %>%
        .[[1]]
      }
  }, error = function(e) {
    shared_df[i, "Duration"] <- NA
    })
  # Extract ticket type
  tryCatch({
    shared_df[i, "Ticket_Type"] <- if (html |>
                                       xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
                                       length() == 0) {
      "Sold out"
      } else {
        html |>
          xml_find_all("//li[@class = 'eds-text-bm eds-text-weight--heavy css-1eys03p']/text()") |>
          xml_text() %>%
          .[[2]]
        }
    }, error = function(e) {
      shared_df[i, "Ticket_Type"] <- NA})
  
  # Extract refund policy
  
  tryCatch({
    shared_df[i, "Refund_Policy"] <- if (html |>
                                         xml_find_all("//div[@class = 'Layout-module__module___2eUcs Layout-module__refundPolicy___fQ8I7']//section[@class = 'event-details__section']/div") |>
                                         length() == 0) {
      NA
      } else {
        html |>
          xml_find_all("//div[@class = 'Layout-module__module___2eUcs Layout-module__refundPolicy___fQ8I7']//section[@class = 'event-details__section']/div") |>
          xml_text() %>%
          .[[2]]
        }
    }, error = function(e) {
      shared_df[i, "Refund_Policy"] <- NA
      })
# Extract description
  tryCatch({
    shared_df[i, "Description"] <- if (html |>
                                       xml_find_all("//div[@class = 'eds-text--left']//p") |>
                                       length() == 0){
      NA
      } else {
        html |>
          xml_find_all("//div[@class = 'eds-text--left']//p") |>
          xml_text() |>
          discard(~.x == "") |>
          paste(collapse = " ")
      }
    }, error = function(e) {
      shared_df[i, "Description"] <- NA
      })
  
  # pull json data
  
  json <-
    html |>
    xml_find_all("//script[@type='application/ld+json']") |>
    pluck(1) |>
    xml_text()
  
  json <- fromJSON(json)
  
  tryCatch({shared_df[i, "LowPrice"] <- json$offers$lowPrice[1]},
           error = function(e) {
             shared_df[i, "LowPrice"] <- NA})
  tryCatch({shared_df[i, "HighPrice"] <- json$offers$highPrice[1]},
           error = function(e) {
             shared_df[i, "HighPrice"] <- NA})
  tryCatch({shared_df[i, "Currency"] <- json$offers$priceCurrency[1]},
           error = function(e) {
             shared_df[i, "Currency"] <- NA})
  tryCatch({shared_df[i, "Organizer"] <- json$organizer$name},
           error = function(e) {
             shared_df[i, "Organizer"] <- NA})
  tryCatch({shared_df[i, "EventStatus"] <- json$eventStatus},
           error = function(e) {
             shared_df[i, "EventStatus"] <- NA})
  tryCatch({shared_df[i, "StartTime"] <- lubridate::as_datetime(json$startDate)},
           error = function(e) {
             shared_df[i, "StartTime"] <- NA})
  tryCatch({shared_df[i, "EndTime"] <- lubridate::as_datetime(json$endDate)},
           error = function(e) {
             shared_df[i, "EndTime"] <- NA})
  tryCatch({shared_df[i, "Title"] <- json$name},
           error = function(e) {
             shared_df[i, "Title"] <- NA})
  tryCatch({shared_df[i, "Subtitle"] <- json$description},
           error = function(e) {
             shared_df[i, "Subtitle"] <- NA})
  tryCatch({shared_df[i, "url"] <- json$url},
           error = function(e) {
             shared_df[i, "url"] <- NA})
  
  shared_df[i, "City"] <- cityname
  
  # calculating and reporting the progress of the scraper
  print(paste(round(100 * j/num_all_links, 2), "% completed"))
  
  res <- try(read_csv(file_path, show_col_types = FALSE), silent = TRUE)
  
  if (inherits(res, "try-error")) {
    # Save the data frame we scraped above
    print("File doesn't exist; Creating it")
    write_csv(shared_df, file_path)
    } else {
      # If the file was read successfully, append the
      # new rows and save the file again
      combined_df <- bind_rows(res, shared_df[i,])
      write_csv(combined_df, file_path)
    }
  rm(res, shared_df)
}
  }

```

# Testing ground 

Load the cities and create url for Madrid

```{r }
l <- create_url("roma")
```


How many pages for Madrid? 

```{r}
count_pages(l)
```

Get all the event info

We don't need to save the funciton to `df <- ...` because it will automatically do it each loop inside the function `get_event_info(l)`. 

```{r write-csv}
get_event_info(l, "roma") 
```
## Appendix

## Pre-convert time format

This function takes the html of an event and prepares it for `convert_time_format()`.

### Filter list

To ensure that 
```{r}
filter_list <- 
  function(list) {
  return(list[sapply(list, function(x) is.character(x) && nchar(x) > 0)])}
```


```{r}
pre_convert_time <- function (html) {if (
    html |> 
    xml_find_all("//div[@class = 'date-info']//span") |>
    length() == 0) {
      NA
  } 
  else {
    html |> 
      xml_find_all("//div[@class = 'date-info']//span") |> 
      xml_text() %>%
      filter_list() %>%
  str_extract_all("\\b(?:\\d+:\\d+(?:pm|PM|am|AM)?|(?:\\d+:)?\\d+pm|\\d+PM|\\d+am|\\d+AM|\\d+ - \\d+:\\d+(?:pm|PM|am|AM)?|\\d+ - \\d+(?:pm|PM|am|AM)?)") %>%
  .[[1]] %>%
  paste0(collapse = " - ")  %>% 
  {if (str_detect(., "\\b\\d{1}(?![0-9]|:)(?:pm|PM|am|AM)")) { # Matches single-digit numbers not followed by another digit or colon, followed by "pm" or "am"
    gsub("\\b(\\d{1})(pm|PM|am|AM)", "0\\1:00\\2", .)
  } else if (str_detect(., "\\b(\\d{1}):?")) { # Matches singular numbers followed by a colon
    gsub("\\b(\\d{1}):", "0\\1:", .)
  } else if (str_detect(., "\\d{1} - \\d{1}(?:pm|PM|am|AM)?")){
    gsub("(\\d{1,2}) - (\\d{1,2})(pm|PM|am|AM)", "0\\1:00 - 0\\2:00\\3", .)
  } else if (str_detect(., "\\d{1,2}(?:pm|PM|am|AM)?")){
    if_else(str_detect(., "pm|PM"), gsub("(\\d{1,2})(pm|PM|am|AM)?", "0\\1:00 - 00:00pm", .), 
            gsub("(\\d{1,2})(pm|PM|am|AM)?", "0\\1:00 - 00:00", .))
  } else if (str_detect(., "\\d{1}")) {
    gsub("(\\d{1})", "0\\1:00", .)
  } else {
    paste(.)
  }
  } %>%
  gsub("\\b(\\d)( - |$)", "0\\1:00\\2", .)%>%
  gsub("^\\b(\\d{2}:\\d{2})(pm|am|PM|AM)$", "\\1 - 00:00\\2", .) %>%
  gsub("^(\\d{2}:\\d{2})$", "\\1 - 00:00\\2", .)%>%
  gsub("(?<=\\s\\d{2})(pm|am|PM|AM)\\b", ":00\\1", ., perl = TRUE)%>%
  gsub("(\\d{1,2})(am|pm|AM|PM)\\s", "\\1:00\\2 ", ., perl = TRUE) %>%
  gsub("\\b(\\d):(\\d{2}(am|pm|AM|PM)?)\\b", "0\\1:\\2", .) %>%
  gsub("(?<!\\d:)(\\b\\d{2})\\b\\s", "\\1:00 ", ., perl = TRUE) %>%
  gsub("(pm|am) - ", " - ", .)%>%
  gsub("(am|AM)$", "pm", .)
  }
  
}
```


## Convert time format 

This function converts a time string into a format usable for the final data table. 

```{r}
convert_time_format <- function(time_string) {
  # Check if "pm" or "PM" is present in the input string
  if (is.na(time_string)) {
    return(NA)
  } else {
    if(str_detect(time_string, "pm|PM")) {
    # Extract hours and minutes from the input string
    time_parts <- str_match(time_string, "(\\d+):(\\d+) - (\\d+):(\\d+)(pm)?")
    start_hour <- as.integer(time_parts[2])
    start_minute <- as.integer(time_parts[3])
    end_hour <- as.integer(time_parts[4])
    end_minute <- as.integer(time_parts[5])
    
    # Convert to 24-hour format
    if (start_hour != 12) {
      start_hour <- start_hour + 12
    }
    
    # Return the start time string in the desired format
    result <- sprintf("%02d:%02d", start_hour, start_minute)
    } else {
      start_time <- str_match(time_string, "(\\d+):(\\d+) - (\\d+):(\\d+)")
      start_hour <- as.integer(start_time[2])
      start_minute <- as.integer(start_time[3])
      
      time_string <- sprintf("%02d:%02d", start_hour, start_minute)
      # Return the original time string if "pm" or "PM" is not present
      result <- time_string
    }
    
    return(result)
  }
}

```

